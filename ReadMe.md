# Тестовое задание на позицию Дата Инженера

ETL-процессы для извлечения данных из предоставленного эксель файла и их послойной  обработки обработки в PostgreSQL через Apache Airflow с поддержкой инкрементальной загрузки.

Проект реализует ETL-процесс :
- Extract — получает данные из randomuser.me/api/
- Transform — фильтрует и форматирует их
- Load — сохраняет в PostgreSQL

### Содержание:
- [Содержание ТЗ](#содержание-тз)
- [Технологии](#технологии)
- [Структура проекта](#структура-проекта)
- [Требования к ПО](#требования-к-по)
- [Инструкция](#инструкция)
- [Контактная информация](#контактная-информация)

## Содержание ТЗ

```
Техническое задание на разработку ETL-процесса и аналитической витрины данных о продажах кофе

Необходимо реализовать процесс обработки данных: от загрузки сырых данных до формирования аналитической витрины с ежедневными метриками продаж кофе.
Техническое задание состоит из двух частей. 

В первой части необходимо реализовать загрузку ETL. Источником будет выступать файл в формате xlsx, где будут храниться данные по продаже кофе. 

Во второй нужно будет провести небольшую аналитику: построить аналитическую витрину в разрезе по дням, где будут храниться основные метрики по продажам в кофе. Какие метрики, должны быть витрине:
1.	Выручка с витрины	
2.	Средний чек
3.	Продажи в единицах (количество проданного)
4.	Топ 1 час, в котором были проданы больше всего кофе за день	

Технические требования:
1.	Использование реляционной СУБД
2.	Обработку некорректных значений (null, дубли)
3.	Реализация схемы данных с разделением на слои: stg (сырые данные), ods (очищенные данные), dm (аналитические витрины)
4.	Наличие технических полей (мета полей)
5.	Поддержка инкрементальной загрузки
```

## Технологии

- Контейниризация: Docker + Docker Compose
- База данных: PostgreSQL
- Реализация ETL-процесса: Python, Redis, Apache Airflow
    в том числе:
    - Airflow-api-server 
    
        ***в прошлом webserver***
    - Airflow worker
    - Airflow-scheduler
    - Airflow-dag-processor
    - Airflow-triggerer 
    
        ***отключен потому что ETL-процесс написан не на asyncio (triggerer работает в асинхроне поэтому если что-то блокирует асинхронный цикл — ты получишь предупреждение)***
    -  Pandas
    
        ***библиотека Python для работы с табличными данными*** 



## Структура проекта

```
PetProject_airflow/
├── dags/             # Папка с DAG'ами
│   ├── dag_stg_coffee_sales_load.py
│   ├── dag_ods_coffee_sales_clean.py
│   └── dag_dm_coffee_sales_aggregate.py
│
├── data/                  # Папка с данными
│   └── coffee_sales.xlsx  # Файл с предоставленными данными
│
├── sql/                  # Кусочки кода требуемы при инициализации БД
│   ├── create_stg.sql  # Схема слоя с сырыми данными
│   ├── create_ods.sql  # Схема слоя с обработанными данными
│   ├── create_dm.sql   # Схема слоя с аналитической витриной
│   └── create_tables.sql # Все вместе
│
├── requirements.txt      # Зависимости Python
├── docker-compose.yml    # Docker Compose конфигурация
├── Dockerfile            # Инструкции для Airflow контейнера
├── .env.sample           # Пример переменных окружения
├── .gitignore            # gitignore
└── README.md             # Это README
```

## Требования к ПО

- Docker, Docker Compose ***[Скачать](https://docs.docker.com/get-started/get-docker/ "В Docker Desktop уже все есть")***
- Git ***[Скачать](https://git-scm.com/downloads)***

## Инструкция

1. Клонировать репозиторий:
```bash
git clone https://github.com/MonkeysTower/TestWorkDataEngineer.git
cd TestWorkDataEngineer/
```

2. Создать .env:
    - Скопривать .env.sample
    - Отредактировать перменные внутри него под себя 
    
        ***Стоит отметить что \_AIRFLOW\_WWW\_USER\_USERNAME \_AIRFLOW\_WWW\_USER\_PASSWORD будут использоваться для входа на платформу Airflow***

3. Собрать контейнеры:
```cmd
docker-compose up
```
или
```cmd
docker-compose up -d
```
***Подождать все равно придется***

4. Зайти на платформу Airflow 
```https
https://localhost:{8081 или указаный вами в .env}
```

5. Запустить DAGи последовательно
    1. Сначала ```load_coffee_raw_once``` поскольку shedule для него не настроен, то сделать это следует через триггер.
    2. Затем ```clean_coffee_ods```
    3. И в конце ```build_coffee_dm```

    ***Аналитическая витрина будет находиться в схеме dm в таблице daily_sales базы данных указаной в .env PostgeSQL (порт, логин, пароль для подключения указывают тоже в .env)***

\[Optional\] Для проверки можно использовать или **pgAdmin** или командную строку (windows):
```
docker exec -it {Номер контейнера с PostgreSQL} psql -U {Имя пользователя из .env} -d {Имя БД из .env} -c "SELECT COUNT(*) FROM dm.daily_sales;"
```
***Чтобы выйти нажмите кнопку 'Q'***

## Контактная информация
Если остались какие-то вопросы или предложения, не стесняйтесь свяжитесь со мной:
 - Email: Trifandre@yandex.ru
 - GitHub: <https://github.com/MonkeysTower/>
